{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning PySpark \n",
    "### Video series\n",
    "\n",
    "### Packt Publishing\n",
    "\n",
    "**Author**: Tomasz Drabas\n",
    "**Date**:   2017-12-10\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Section 2: Resilient Distributed Datasets - Transformations\n",
    "\n",
    "In this section we will look at the Resilient Distributed Datasets (RDDs) and the transformations available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating RDDs\n",
    "### Parallelize a collection\n",
    "#### 1000 random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>Current session?</th></tr><tr><td>3</td><td>None</td><td>pyspark</td><td>idle</td><td></td><td></td><td>âœ”</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n",
      "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "random_numbers = sc.parallelize([np.random.rand() for _ in range(1000)], 4)\n",
    "random_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5556534656821962, 0.3949968193806832]"
     ]
    }
   ],
   "source": [
    "random_numbers.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mark', 43, '6\\'1\"'), ('Stella', 23, '5\\'6\"')]"
     ]
    }
   ],
   "source": [
    "rdd_random_tuples = sc.parallelize([\n",
    "      (\"Mark\",  43, \"6'1\\\"\")\n",
    "    , (\"Stella\",23, \"5'6\\\"\")\n",
    "    , (\"Skye\",   6, \"3'11\\\"\")\n",
    "    , (\"Albert\", 1, \"2'7\\\"\")\n",
    "])\n",
    "rdd_random_tuples.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading from files\n",
    "The `sample_data.csv` was created based on the data found here: http://www.contextures.com/xlSampleData01.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderDate,Region,Rep,Item,Units,UnitCost,Total', '1/6/16,East,Jones,Pencil,95,1.99,189.05']"
     ]
    }
   ],
   "source": [
    "rdd_from_file = sc.textFile('../data/sample_data.csv', 4)\n",
    "rdd_from_file.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema of an RDD\n",
    "### Structured data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_valid_structured = sc.parallelize([\n",
    "      ('Tom', 0)\n",
    "    , ('Spark', 1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unstructured data\n",
    "Sample Linux log data, source: https://www.cyberciti.biz/faq/linux-log-files-location-and-how-do-i-view-logs-files/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_valid_unstructured = sc.parallelize([\n",
    "      ('Jul 17 22:04:25 router  dnsprobe[276]: dns query failed')\n",
    "    , ('Jul 17 22:04:29 router last message repeated 2 times')  \n",
    "    , ('Jul 17 22:04:29 router  dnsprobe[276]: Primary DNS server Is Down... Switching To Secondary DNS server')\n",
    "    , ('Jul 17 22:05:08 router  dnsprobe[276]: Switching Back To Primary DNS server')\n",
    "    , ('Jul 17 22:26:11 debian -- MARK --')\n",
    "    , ('Jul 17 22:46:11 debian -- MARK --')\n",
    "    , ('Jul 17 22:47:36 router  -- MARK --')\n",
    "    , ('Jul 17 22:47:36 router  dnsprobe[276]: dns query failed')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd_valid_heterogeneous = sc.parallelize([\n",
    "      ('Toll invoice number', 'TRE2321')\n",
    "    , {'Last bill balance': 0.00, 'New bill balance': 23.92}\n",
    "    , ['2017-11-12 12:32:34PM', '2017-11-14 1:32:56PM']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding lazy execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names_only = rdd_valid_structured.map(lambda element: element[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_only.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .map(...) transformation\n",
    "### Using lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderDate', 'Region', 'Rep', 'Item', 'Units', 'UnitCost', 'Total']\n",
      "['1/6/16', 'East', 'Jones', 'Pencil', '95', '1.99', '189.05']"
     ]
    }
   ],
   "source": [
    "def splitOnComma(inputString):\n",
    "    return inputString.split(',')\n",
    "\n",
    "for el in rdd_from_file.map(splitOnComma).take(2):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['OrderDate', 'Region', 'Rep', 'Item', 'Units', 'UnitCost', 'Total']\n",
      "['1/6/16', 'East', 'Jones', 'Pencil', '95', '1.99', '189.05']"
     ]
    }
   ],
   "source": [
    "for el in rdd_from_file.map(lambda element: element.split(',')).take(2):\n",
    "    print(el)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transforming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['OrderDate', 'Region', 'Rep', 'Item', -666, -666, -666], ['1/6/16', 'East', 'Jones', 'Pencil', 95.0, 1.99, 189.05]]"
     ]
    }
   ],
   "source": [
    "def convertToFloat(inputString):\n",
    "    try:\n",
    "        return float(inputString)\n",
    "    except:\n",
    "        return -666\n",
    "\n",
    "to_filter = rdd_from_file \\\n",
    "    .map(lambda element: element.split(',')) \\\n",
    "    .map(lambda element:\n",
    "         [e for e in element[:4]] +\n",
    "         [convertToFloat(e) for e in element[4:]]\n",
    "    )\n",
    "    \n",
    "to_filter.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .filter(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1/6/16', 'East', 'Jones', 'Pencil', 95.0, 1.99, 189.05], ['2017-03-02', 'Central', 'Kivell', 'Binder', 50.0, 19.99, 999.5]]"
     ]
    }
   ],
   "source": [
    "filtered = to_filter.filter(lambda element: element[4] != -666)\n",
    "filtered.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .flatMap(...) transformation\n",
    "### .map to flatMap comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 30], [2, 40], [3, 50], [4, 60]]\n",
      "[1, 30, 2, 40, 3, 50, 4, 60]"
     ]
    }
   ],
   "source": [
    "def mapExample(inputList):\n",
    "    outputList = []\n",
    "    \n",
    "    for item in inputList:\n",
    "        temp = item.copy()\n",
    "        temp[1] *= 10\n",
    "        \n",
    "        outputList.append(temp)\n",
    "    return outputList\n",
    "\n",
    "def flatMapExample(inputList):\n",
    "    outputList = []\n",
    "    \n",
    "    for item in inputList:\n",
    "        temp = item.copy()\n",
    "        temp[1] *= 10\n",
    "        \n",
    "        outputList += temp\n",
    "        \n",
    "    return outputList\n",
    "\n",
    "sampleList = [\n",
    "      [1, 3]\n",
    "    , [2, 4]\n",
    "    , [3, 5]\n",
    "    , [4, 6]\n",
    "]\n",
    "\n",
    "print(mapExample(sampleList))\n",
    "print(flatMapExample(sampleList))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering malformed records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [[datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05]], []]"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "def parseCSVRow(inputRow):\n",
    "    try:\n",
    "        rowSplit = inputRow.split(',')\n",
    "        rowSplit[0] = dt.datetime.strptime(rowSplit[0], '%m/%d/%y')\n",
    "        rowSplit[4] = int(rowSplit[4])\n",
    "        \n",
    "        for i in [5,6]:\n",
    "            rowSplit[i] = float(rowSplit[i])\n",
    "        \n",
    "        return [rowSplit]\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "rdd_from_file.map(parseCSVRow).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05]], [[datetime.datetime(2016, 2, 9, 0, 0), 'Central', 'Jardine', 'Pencil', 36, 4.99, 179.64]], [[datetime.datetime(2016, 2, 26, 0, 0), 'Central', 'Gill', 'Pen', 27, 19.99, 539.73]]]"
     ]
    }
   ],
   "source": [
    "rdd_from_file.map(parseCSVRow).filter(lambda element: len(element) > 0).take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05], [datetime.datetime(2016, 2, 9, 0, 0), 'Central', 'Jardine', 'Pencil', 36, 4.99, 179.64], [datetime.datetime(2016, 2, 26, 0, 0), 'Central', 'Gill', 'Pen', 27, 19.99, 539.73]]"
     ]
    }
   ],
   "source": [
    "rdd_from_file_clean = rdd_from_file.flatMap(parseCSVRow)\n",
    "rdd_from_file_clean.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .distinct(...) transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the distinct method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 3, 6, 8, 9]"
     ]
    }
   ],
   "source": [
    "sampleList = [1,1,1,4,3,4,6,4,8,6,9]\n",
    "\n",
    "distinct = []\n",
    "seen = {}\n",
    "\n",
    "for elem in sampleList:\n",
    "    if elem in seen:\n",
    "        continue\n",
    "    else:\n",
    "        distinct.append(elem)\n",
    "        seen[elem] = 1\n",
    "    \n",
    "print(distinct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding distinct in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Desk', 'Pen', 'Pencil', 'Pen Set', 'Binder']"
     ]
    }
   ],
   "source": [
    "items = rdd_from_file_clean.map(lambda element: element[3]).distinct()\n",
    "items.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11"
     ]
    }
   ],
   "source": [
    "rdd_from_file_clean.map(lambda element: element[2]).distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .sample(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42"
     ]
    }
   ],
   "source": [
    "rdd_from_file_clean.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9"
     ]
    }
   ],
   "source": [
    "rdd_from_file_clean.sample(False, 0.2).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .join(...) transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The mechanics of join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('b', (3, 4)), ('a', (2, 4))]"
     ]
    }
   ],
   "source": [
    "a = sc.parallelize([('a', 2), ('b', 3)])\n",
    "b = sc.parallelize([('a', 4), ('b', 4), ('c', 4)])\n",
    "\n",
    "a.join(b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Append location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[datetime.datetime(2016, 2, 9, 0, 0), 'Central', 'Jardine', 'Pencil', 36, 4.99, 179.64, 'Chicago'], [datetime.datetime(2016, 2, 26, 0, 0), 'Central', 'Gill', 'Pen', 27, 19.99, 539.73, 'Chicago']]"
     ]
    }
   ],
   "source": [
    "cities = sc.parallelize([\n",
    "      ('East', 'Boston')\n",
    "    , ('Central', 'Chicago')\n",
    "    , ('West', 'Seattle')\n",
    "])\n",
    "\n",
    "rdd_from_file_clean \\\n",
    "    .map(lambda element: (element[1], element)) \\\n",
    "    .join(cities) \\\n",
    "    .map(lambda element: element[1][0] + [element[1][1]]) \\\n",
    "    .take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .repartition(...) transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4"
     ]
    }
   ],
   "source": [
    "rdd_from_file_clean.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2"
     ]
    }
   ],
   "source": [
    "rdd_from_file_repartitioned = rdd_from_file.repartition(2)\n",
    "rdd_from_file_repartitioned.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(datetime.datetime(2017, 10, 31, 0, 0), 'Central', 'Andrews', 'Pencil', 14, 1.29, 18.06), (datetime.datetime(2017, 11, 17, 0, 0), 'Central', 'Jardine', 'Binder', 11, 4.99, 54.89), (datetime.datetime(2017, 5, 14, 0, 0), 'Central', 'Gill', 'Pencil', 53, 1.29, 68.37), (datetime.datetime(2016, 12, 12, 0, 0), 'Central', 'Smith', 'Pencil', 67, 1.29, 86.43), (datetime.datetime(2016, 8, 15, 0, 0), 'East', 'Jones', 'Pencil', 35, 4.99, 174.65), (datetime.datetime(2016, 9, 1, 0, 0), 'Central', 'Smith', 'Desk', 2, 125.0, 250.0), (datetime.datetime(2017, 7, 21, 0, 0), 'Central', 'Morgan', 'Pen Set', 55, 12.49, 686.95)], [(datetime.datetime(2017, 9, 10, 0, 0), 'Central', 'Gill', 'Pencil', 7, 1.29, 9.03), (datetime.datetime(2017, 2, 18, 0, 0), 'East', 'Jones', 'Binder', 4, 4.99, 19.96), (datetime.datetime(2016, 7, 12, 0, 0), 'East', 'Howard', 'Binder', 29, 1.99, 57.71), (datetime.datetime(2016, 5, 22, 0, 0), 'West', 'Thompson', 'Pencil', 32, 1.99, 63.68), (datetime.datetime(2017, 4, 10, 0, 0), 'Central', 'Andrews', 'Pencil', 66, 1.99, 131.34), (datetime.datetime(2017, 3, 7, 0, 0), 'West', 'Sorvino', 'Binder', 7, 19.99, 139.93), (datetime.datetime(2017, 12, 21, 0, 0), 'Central', 'Andrews', 'Binder', 28, 4.99, 139.72), (datetime.datetime(2016, 4, 18, 0, 0), 'Central', 'Andrews', 'Pencil', 75, 1.99, 149.25), (datetime.datetime(2017, 9, 27, 0, 0), 'West', 'Sorvino', 'Pen', 76, 1.99, 151.24), (datetime.datetime(2016, 3, 15, 0, 0), 'West', 'Sorvino', 'Pencil', 56, 2.99, 167.44), (datetime.datetime(2016, 2, 9, 0, 0), 'Central', 'Jardine', 'Pencil', 36, 4.99, 179.64), (datetime.datetime(2016, 1, 6, 0, 0), 'East', 'Jones', 'Pencil', 95, 1.99, 189.05), (datetime.datetime(2017, 3, 24, 0, 0), 'Central', 'Jardine', 'Pen Set', 50, 4.99, 249.5), (datetime.datetime(2016, 10, 5, 0, 0), 'Central', 'Morgan', 'Binder', 28, 8.99, 251.72), (datetime.datetime(2016, 9, 18, 0, 0), 'East', 'Jones', 'Pen Set', 16, 15.99, 255.84), (datetime.datetime(2016, 4, 1, 0, 0), 'East', 'Jones', 'Binder', 60, 4.99, 299.4), (datetime.datetime(2016, 11, 8, 0, 0), 'East', 'Parent', 'Pen', 15, 19.99, 299.85), (datetime.datetime(2017, 7, 4, 0, 0), 'East', 'Jones', 'Pen Set', 62, 4.99, 309.38), (datetime.datetime(2017, 1, 15, 0, 0), 'Central', 'Gill', 'Binder', 46, 8.99, 413.54), (datetime.datetime(2016, 5, 5, 0, 0), 'Central', 'Jardine', 'Pencil', 90, 4.99, 449.1), (datetime.datetime(2016, 6, 25, 0, 0), 'Central', 'Morgan', 'Pencil', 90, 4.99, 449.1), (datetime.datetime(2016, 11, 25, 0, 0), 'Central', 'Kivell', 'Pen Set', 96, 4.99, 479.04), (datetime.datetime(2017, 4, 27, 0, 0), 'East', 'Howard', 'Pen', 96, 4.99, 479.04), (datetime.datetime(2016, 2, 26, 0, 0), 'Central', 'Gill', 'Pen', 27, 19.99, 539.73), (datetime.datetime(2016, 6, 8, 0, 0), 'East', 'Jones', 'Binder', 60, 8.99, 539.4), (datetime.datetime(2016, 10, 22, 0, 0), 'East', 'Jones', 'Pen', 64, 8.99, 575.36), (datetime.datetime(2017, 6, 17, 0, 0), 'Central', 'Kivell', 'Desk', 5, 125.0, 625.0), (datetime.datetime(2017, 5, 31, 0, 0), 'Central', 'Gill', 'Binder', 80, 8.99, 719.2), (datetime.datetime(2017, 8, 24, 0, 0), 'West', 'Sorvino', 'Desk', 3, 275.0, 825.0), (datetime.datetime(2017, 8, 7, 0, 0), 'Central', 'Kivell', 'Pen Set', 42, 23.95, 1005.9), (datetime.datetime(2017, 10, 14, 0, 0), 'West', 'Thompson', 'Binder', 57, 19.99, 1139.43), (datetime.datetime(2016, 12, 29, 0, 0), 'East', 'Parent', 'Pen Set', 74, 15.99, 1183.26), (datetime.datetime(2017, 2, 1, 0, 0), 'Central', 'Smith', 'Binder', 87, 15.0, 1305.0), (datetime.datetime(2016, 7, 29, 0, 0), 'East', 'Parent', 'Binder', 81, 19.99, 1619.19), (datetime.datetime(2017, 12, 4, 0, 0), 'Central', 'Jardine', 'Binder', 94, 19.99, 1879.06)]]"
     ]
    }
   ],
   "source": [
    "rdd_from_file_repartitioned_sorted = rdd_from_file_clean \\\n",
    "    .map(lambda element: (int(element[6]), element)) \\\n",
    "    .repartitionAndSortWithinPartitions(2, lambda x: x) \\\n",
    "    .map(lambda element: tuple(element[1]))\n",
    "\n",
    "rdd_from_file_repartitioned_sorted.glom() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_from_file_repartitioned_sorted.getNumPartitions()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
